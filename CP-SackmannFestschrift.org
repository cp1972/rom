#+Bibliography: /home/cpsoz/Github/rom/CP-ROM.bib

* Einleitung

Im Oeuvre Reinhold Sackmanns hat uns einen Ansatz besonders interessiert, den er im Rahmen seiner unterschiedlichen Arbeiten zur Demographie, zur Lebenslaufsoziologie und zu gesellschaftlichen Instanzen der Sozialisation wie den (Hoch)Schulen, dem Arbeitsmarkt oder dem Rentensystem entwickelt hat. Dieser Ansatz finden wir in seinen Arbeiten unterschiedlich ausgedruckt. Nehmen wir etwa die Schlussfolgerungen seiner Habilitationsschrift, wenn er eine Bilanz zur Lebenslaufsoziologie wie folgt zieht: "An den Anfängen dieser Disziplin (...) stand die Vorstellung, daß Alter neben seiner biologischen Komponente auch gesellschaftlich kulturelle Elemente enthält. Gesellschaft war ein Nebenfaktor, der wahrgenommen werden sollte. In der Theorie des institutionalisierten Lebenslaufs gewann die Vorstellung Kontur, daß die modernen gesellschaftlichen Institutionen (...) spezifische Lebensläufe kontitutieren" [cite:@sackmann98:_konkur_gener_arbeit, 228-229]. Dabei legt Sackmann nah, dass sich die künftige Forschung in der Lebenslaufsoziologie mit der Art und Weise auseinandersetzen sollte, wie Lebensläufe als eingebettete Prozesse an der Schnittstelle der Biographien von Akteuren und der formalen Operationen von gesellschaftlichen Instanzen entstehen und von einer solchen Einbettung strukturiert werden. "Lebensläufe", wie er in einem späteren Beitrag sagt, "vollziehen sich eingebettet, in Gruppen, in Gemeinschaften, Gesellschaften und v.a. Organisationen" [cite:@sackmann07:_leben_biogr, 206]. Dieser Ansatz ist aus den folgenden Gründen interessant.

Sackmann versteht Lebensläufe nicht mehr im geläufigen soziologischen Sinn von "trajectoires", die den Fokus auf den Weg der Akteure von gesellschaftlichen Instanzen der Sozialisation zu weiteren gesellschaftlichen Instanzen der Sozialisation setzen. Damit wird gesagt, das gesellschaftliche Sozialisationsinstanzen wie etwa (Hoch)Schulen oder Arbeitsmarkt mehr als nur "verpflichtende Anlaufstationen" von Akteuren sind [cite:@latour05:_reass_social], die Akteure registrieren würden und je nach Sozialisationserfolg unterschiedlich auf weitere gesellschaftliche Bahnen umverteilen würden. Gesellschaftliche Instanzen sind keine irreflexive Orte der Vergebung von Titeln und Rechten, sondern sie geben den Akteuren Einschreibungsmöglichkeiten in der Gesellschaft und reflektieren entsprechend über ihre Rolle in diesem Zusammenhang. Auch zu diesem Zweck entwickeln gesellschaftliche Instanzen dauernd formale Verfahren, die Orientierungsmuster bzw. "Leitbilder" für die Akteure schaffen und deshalb auf ihre Laufbahnen wirken [cite:@sackmann07:_leben_biogr, 206]. Dass eine solche Wirkung auch unintendierte Folgen hat und Ungleichheiten zwischen Akteuren bzw. Akteurgenerationen generiert, hat Sackmann mehrmals im Sinne einer kritischen soziologischen Betrachtung von gesellschaftlichen Instanzen unterstrichen [cite: vgl. etwa @sackmann15:_bedeut_auswah_erzeug_bildun; vgl. auch @sackmann19:_mechan_eliteb]. Aber eine kritische Betrachtung von gesellschaftlichen Instanzen reicht allein nicht aus. Nach Sackmann müssen ebenfalls Korrekturmaßnahmen von gesellschaftlichen Instanzen abgebildet werden, die darauf abzielen, Maßnahmen und entsprechende formale Verfahren zur Einschränkungen bzw. Schrumpfung der Ungleichheitsproduktion zu definieren.

Mit diesem konstruktiven Korrelat zur kritischen Betrachtung von gesellschaftlichen Instanzen bringt Sackmann seinen Ansatz zur Vollendung: Weil Kollektivakteure als Teil des Einschreibungskontextes fungieren, den sie mit den Akteuren aufbauen und innerhalb dessen die Laufbahn von solchen Akteuren strukturiert wird, ist es aus Sicht einer soziologischen Makroperspektive nicht ausreichend, nur die Defizite von Kollektivakteuren in ihren Wechselwirkungen mit Einzelakteuren hervorzuheben. Es muss ebenfalls verstanden werden, wie gesellschaftliche Instanzen an ihren formalen Operationen Arbeiten, um sie nicht nur im Sinne der Stärkung der eigenen Position in der Gesellschaft zu verbessern, sondern auch im Sinne der Förderung des Lebenslaufs ihrer Adressaten bzw. der Zuverlässigkeit von (positiven wie negativen) Sanktionsoperationen an der Wurzel der (Um)Verteilung von Akteuren auf gesellschaftliche Tätigkeiten, entsprechende Bereiche und dort aktive Instanzen. Aus einem solchen Ansatz erkennen wir zwei wichtigen Botschaften.

Die erste Botschaft betrifft den gesellschaftlichen Zusammenhalt. Der gesellschaftliche Zusammenhalt ist kein Zustand, sondern eine Dynamik [cite:@sackmann98:_konkur_gener_arbeit, 82], die eine eigene Elastizität je nachdem hat, wie gut oder weniger gut die Lebensläufe durch die Einbettung von Akteuren und gesellschaftlichen Instanzen strukturiert werden. In seinem Ansatz denkt Sackmann eine solche Dynamik mit dem Begriff der Figuration von Norbert Elias, von dem er insbesondere die Interdependenz der protagonistischen und antagonistischen Akteuren in Figurationen sowie die Machtbalancen hervorhebt (ebd., bes. 73ff.). Figurationen sind keine Zustände, sondern Zusammenstellungen von Einzel- wie Kollektiveakteuren und Nicht-Akteuren, die sich in Zeit und Raum mehr oder weniger verbreiten oder schrumpfen. Die zweite Botschaft betrifft diese Verbreitung bzw. Schrumpfung. Im Rahmen von Figuration denkt sie Elias als von Machtbalancen innerhalb von Figuration sowie von den Machtbalancen zwischen Figurationen abhängig, was bedeutet, dass (Kollektiv)Akteure in Figuration "nicht gleich stark" sind (ebd. 75), weshalb auch Asymmetrien entstehen, die jedoch keine radikalen Asymmetrien im Sinne der Monopolisierung von Macht bilden: "selten gelingt es einer Konfliktpartei, die andere vollkommen zu entmachten" (ebd.). Die Macht wird in Figurationen eng mit der Interdependenz von den unterschiedlichen (Kollektiv)Akteuren verbunden und somit dynamisiert, was im Begriff "Balance" wiedergegeben wird -- die Macht ist im Wechselspiel der (Kollektiv)Akteuren eingebettet und wird im Laufe der Strukturierung der Lebensläufe von Einzelakteuren und der Position von gesellschaftlichen Instanzen mitstrukturiert. Einmal figurationssoziologisch aufgefasst, erscheinen Lebensläufe von Einzelakteuren und Generationen von Akteuren nicht mehr als individuelle Mikrophänomene "neben" der Gesellschaft als Makrophänomen. Eine solche Unterscheidung kann zugunsten von einem prozeduralen Verständnis von Figurationen als Einbettungs- bzw. Einschreibungskontexte von unterschiedlichen gesellschaftlichen Dimensionen ersetzt werden. Damit stellt sich die folgende analytische Frage: Was haben wir für Figurationen in unserer Gesellschaft, und welche Folgen haben sie für die Handlung in dieser Gesellschaft und schließlich für diese Gesellschaft selbst?

Eine solche Frage teilen wir im Rahmen der Theorie der Relation, die Relation nach einem Makroansatz definiert und entwickelt [cite:@papilloud22:_skizz_theor_relat]. Verglichen mit dem figurationssoziologischen Ansatz Sackmanns geht es in diesem Theorierahmen darum, die Einbettungszusammenhänge radikaler als Relationsstrukturen zu konzipieren, die Ordnungen von Verhältnissen mit (Kollektiv)Akteuren, Nicht-Akteuren und Medien darstellen, die sich nach einer spezifischen Dynamik entwickeln bzw. die verbreitet oder geschrumpft werden. Wichtig dabei bleibt der Ansatz, den wir im Rahmen Sackmanns Deutung der Figuration Elias' zusammengefasst haben: Relationsstrukturen stellen stratifizierte Einbettungs- bzw. Einschreibungskontexte von gesellschaftlichen Phänomenen, Akteuren und Kollektiven in der Form von Verhältnissen dar, die nach einer spezifischen Dynamik der Balance von all möglichen Verhältnissen strukturiert werden, was wiederum eine solche Dynamik strukturiert bzw. verbreitet, be- oder entschleunigt, schrumpfen lässt oder korrigiert. In einem solchen Rahmen bewegt sich die nachfolgende einführende Untersuchung vom Kollektivakteur Rechenzentren, mit der wir einen Beitrag zur Frage leisten wollen, welche Machtbalance Rechenzentren in unserer Gesellschaft in den Vordergrund bringen und welche Folgen entstehen daraus für weitere Ordnungen von Verhältnissen. Als erster Schritt in dieser Untersuchung wollen wir verstehen, was Rechenzentren für eine Art von gesellschaftlicher Instanz sind.

* Rechenzentren verorten

Ein erstes Problem, das den Rahmen der Theorie der Relation lösen will, betrifft die Diskrepanz zwischen dem Realen und dem Formalen bei der Verortung von Instanzen der Gesellschaft bzw. bei der Beschreibung der Position von gesellschaftlichen Instanzen in Ordnungen von Verhältnissen. Real können wir Instanzen und Instanzfraktionen in unterschiedlichen Orten der Gesellschaft finden. Nehmen wir das Beispiel der Forschung: Sie kann als akademische Forschung im Rahmen von Hochschulen oder von Forschungszentren stattfinden, aber sie kann auch in Abteilungen von Industrien oder Firmen sowie auch im Rahmen der Politikberatung oder des Journalismus praktisiert werden. Strukturell setzt sie gleiche Rahmenbedingungen überall in Bezug auf die Mittel sowie auf die Zwecken der Forschung voraus: einen Zugang zu einer zuverlässigen Information, die durch Einbezug von weiteren Akteuren, Kollektiven, Nicht-Akteuren und Medien kontrolliert und vertieft werden kann und eine breite oder weniger breit Öffentlichkeit erreichen muss. Forschung kann folglich überall in der Gesellschaft auftauchen. Aber nicht überall in dieser Gesellschaft ist Forschung gleich Forschung, sondern es hängt davon ab, welche Ordnung von Verhältnisse Forschung einerseits strukturiert und Forschung andererseits unterstützt. Damit wird verstanden, dass die Forschung an Universitäten im Vergleich etwa zur Forschung in Firmen nicht besser/schlechter, wichtiger/weniger wichtig, schneller/langsamer, stärker/schwächer wäre, sondern Verhältnisse zwischen Akteuren, Instanzen, Nicht-Akteuren und Medien in der Forschung anders gestaltet bzw. eine andere Relationalität oder relationale Dynamik zwischen ihnen entwickelt. Daraus ergibt sich, dass die relationale Dynamik einer Ordnung von Verhältnissen als Leitbild für die Entwicklung der gesellschaftlichen Zirkulation von Akteuren und die Positionierung bzw. Verortung von gesellschaftlichen Instanzen funktioniert und ihre Handlung entsprechend beeinflusst. Diese Überlegung kann gleichermaßen zwischen Ordnungen von Verhältnissen bzw. Relationsstrukturen entwickelt werden: eine Ordnung von Verhältnissen versucht stets, die anderen Ordnungen von Verhältnissen über ihre eigenen Instanzen nach ihrer eigenen Dynamik zu beeinflussen, um sich auf die anderen Ordnungen von Verhältnissen so zu verbreiten, als ob diese anderen Ordnungen von Verhältnissen Merkmale ihrer Dynamik wären. Im Zusammenhang dieses Beitrages wollen wir die zwei Anhaltspunkte dieser Überlegung miteinander enger verbinden, indem wir ausgehend von der Verortung der Rechenzentren in Ordnungen von Verhältnissen zeigen, wie sie andere Ordnungen von Verhältnissen zu beeinflussen beabsichtigen und auf welche Schwierigkeiten sie treffen.

Aus unserer Ausführung am Beispiel der Forschung kann schlussfolgert werden, dass Rechenzentren nicht zwangsläufig in der medialen Umwelt der Gesellschaft angesiedelt werden oder, in den Begriffen der Theorie der Relation, dass wir sie nicht a priori als Instanzen einer medialen Ordnung von Verhältnissen bzw. einer medialen Relationsstruktur identifizieren. Rechenzentren sind nicht gleich Rechenzentren je nach dem Bereich der Gesellschaft, den wir uns anschauen bzw. je nach der Einbettung von Verhältnissen, davon sie einerseits ein Ergebnis sind und deren Entwicklung sie unterstützen. Diese Anmerkung führt zur weiteren Einschränkung von unserem Thema: Wir fokussieren die Rechenzentren, die als Instanz der medialen Relationsstruktur gelten. Nach dem Ansatz der Theorie der Relation sind solche Instanzen diejenige, die an der Optimierung der Attraktivität von Medien in allen Formen von Verhältnissen und deshalb an der Optimierung von solchen medialen Verhältnissen arbeiten. Diese Arbeit setzt eine Beschäftigung mit den materiellen Bedingungen von solchen medialen Verhältnissen voraus, um die Investition in solche Verhältnisse zu vermehren und die Mobilisierung von solchen Verhältnissen in der Gesellschaft zu intensivieren. So kann die Repräsentation von solchen medialen Verhältnissen und deren entsprechenden Media in der Gesellschaft sowie ihre Legitimität als diese Verhältnisse und Medien gestärkt werden, die möglicherweise in der ganzen Gesellschaft verbreitet und andere Verhältnisse nach diesem Muster strukturieren werden.

Die erste Bedingung zur Verortung von Rechenzentren in der medialen Relationsstruktur ist in der Literatur zu den Rechenzentren gut abgebildet, selbst wenn hier die Literatur differenziert betrachtet werden muss. Sucht man nach wissenschaftlichen Beiträgen zu Rechenzentren als Gegenstände der soziologischen bzw. sozialwissenschaftlichen Untersuchung, dann merkt man dass Rechenzentren in Verbindung mit einem Ansatz untersucht werden, der auf die materiellen Grundlagen von Rechenoperationen zurückgreift. Dabei wird die Botschaft geliefert, dass die abstrakte bzw. virtuelle Welt der Informationsgesellschaft viele konkrete Elemente voraussetzt, ohne die sie gar nicht existieren könnte [cite:vgl. etwa @blanchette11:_mater_histor_bits;@mattern13:_code_clay_data_dirt;@parks15:_signal_traff;@starosielski15:_under_networ]. Wenn dieser Ansatz erst seit der letzten Dekade in den Sozialwissenschaften aufgegriffen wird, die sich vermehrt mit infrastrukturellen Bedingungen der Informationsgesellschaft beschäftigen, ruht er auf einer Fülle von Beiträgen aus der Literatur in den Bereichen der Technikwissenschaften (Ingenieurwissenschaft, Informatik, Physik), die sich speziell mit den Rechenzentren als Zusammenstellung von unterschiedlichen Techniken und als Kern der Informations- und Kommunikationstechnologien (IKT) auseinandersetzt. In diesen Bereichen sind materielle Bedingungen der Informationsgesellschaft von wichtiger Bedeutung nicht zufällig, sondern aufgrund der Orientierung dieser Bereiche an der Optimierung von Raum nicht im klassischen Sinne der geographischen Fläche, sondern zuerst im Sinne der Miniaturisierung von technischen Komponenten und im Allgemeinen von Medien. In vergangenen Arbeiten haben wir Untersuchungen zu diesem Ansatz vorgelegt, der nach dem Zweiten Weltkrieg im Rahmen der Entstehung von interaktiven Medien und der Entwicklung von Nanotechnologien in Verbindung mit unterschiedlichen Branchen der Wirtschaft (Kommunikationstechnologien, Lebensmittel) und Disziplinen der Wissenschaft (Medizin) entwickelt wird [cite:@monoPapilloud2007;@monoPapilloud2012;@kapitelPapilloud2013;@monoPapilloudSchultze2023]. Dabei handelt es sich darum, neue Räumlichkeiten durch die Miniaturisierung von materiellen Oberflächen für entsprechende miniaturisierten Technologien zu schaffen, die dann auf diesen Oberflächen vermehrt werden könnten. Insbesondere im Zusammenhang der Nanotechnologien versprechen Autoren eine neue industrielle Revolution durch eine solche radikale Miniaturisierung von Technologien, die mit sich eine Umverteilung von Räumlichkeiten bringt. Was da draußen als Raum beobachtet werden kann, wird dank der Eroberung von Räumlichkeiten innerhalb von Materien erspart. Mit diesem Ansatz ist der spezielle Ansatz, der den Diskurs der Expertenliteratur zu Rechenzentren strukturiert, eng verbunden. Er besagt, dass die Umverteilung von Räumlichkeiten zu einer Umverteilung von Energieressourcen führt: Wenn mehr Raum innerhalb von Materien für Technologien gefunden werden kann, dann können Technologien vermehrt werden, ohne mehr Räumlichkeiten in der Welt zu beanspruchen und ohne entsprechende zusätzliche Energieressourcen zu verlangen. Mehr (Technologien) führt zu weniger (Raum- und Energieverbrauch), was als eine negative Formulierung vom Mythos der Abundanz verstanden werden kann. Dieser speziellen Ansatz wird in der Literatur zu Rechenzentren immer bedeutsamer, wenn es verstanden wird, dass Rechenzentren den Kern von IKT und damit von der Informationsgesellschaft bilden.

* Rechenzentren als Kern der Informationsgesellschaft

Dass Rechenzentren den Kern der Informationsgesellschaft bilden, ist im Laufe der Zeit eine Selbstverständlichkeit geworden, die damals in der Gesellschaft vor dem Internet keine war. Die Arbeiten von Bernard Aebischer (ETH-Zürich) im Rahmen des /Centre for Energy Policy and Economics/ liefern in dieser Hinsicht wertvolle Quellen zur Rekonstruktion dieser Entwicklung. Aebischer arbeitet nicht speziell zu Rechenzentren, sondern er untersucht das breite Spektrum der IKT in historischer und transnationaler Perspektive mit einem Modellierungsansatz. Aus dieser Arbeit gewinnt man eine Periodisierung der Entwicklung von IKT und eine Zeitspanne, ab der Rechenzentren immer mehr an Bedeutung für die Informationsgesellschaft gewinnen. Maßnahmen zur Einsparung von Energie werden unmittelbar nach den zwei Ölkrisen von 1973 und 1979 international eingeleitet [cite:@aebischer14:_energ_deman_ict, 6]. Strom wird teuer, deshalb wollen die Länder wissen, wie sie Strom sparen können. Im Laufe der 80er Jahre werden Rechner in der Form von /Personal Computer/ vermehrt in der Wirtschaft und in der Wissenschaft verwendet, was die ersten Studien zum Energieverbrauch von Rechnern in der Gesellschaft motiviert (ebd., 4). Diese Studien arbeiten mit einem Technologieansatz, der vom Moores Gesetz beeinflusst wird, nach dem die Anzahl an Transistoren in integrierten Schaltkreisen jede zwei Jahre verdoppelt wird (ebd.). Sie kommen zu vergleichbaren Ergebnissen. Der Energieverbrauch in der Gesellschaft steigt durch die vermehrte Verwendung von Rechnern. Aber mit der technologischen Entwicklung soll er schnell wieder senken. Simulationen und Szenarien, die in dieser Zeit von 1980 und 1990 für eine Zeitspanne von bis 40 Jahren gebildet werden (1985-2025), kommen zum Schluss, dass es gute Chancen gibt, dass in 2025 Rechner insgesamt nicht mehr Energie als im Jahr 1985 verbrauchen würden (etwa am Beispiel der Schweiz, ebd.). Dabei werden auch sog. /mainframes/ bzw. existierende Rechenzentren mitberücksichtigt, selbst wenn sie keine spezielle Hervorhebungen bekommen. Wie Aebischer unterstreicht, sind wir in einer Gesellschaft vor dem Internet: "the financial sector, research institutions and universities, a few industries and some governmental agencies were the only important users of large computers. Electricity demand of these machines was a concern for these organizations but was barely registered by policy makers or the general public" (ebd., 11). Um genauer zu wissen, was Aebischer mit "barely registered by policy makers" meint, können wir das Beispiel Deutschland heranziehen. Dieses Beispiel bietet den zusätzlichen Vorteil, dass wir damit auf einen methodologischen Ansatz bei Sackmann zurückgreifen, der eine Integration von qualitativer und quantitativer Forschung auf der Grundlage von Topic-Modellen vorsieht, daran Sackmann u.a. auch mit unserer Anwendung /MTA/ bildet [cite:@sackmann20:_sozial_zusam_pandem;@papilloud17:_mta_multi_text_analy].

In der Zwischenzeit ist diese Methode in den Sozialwissenschaften rezipiert worden, weshalb wir sie hier nicht nochmal darstellen werden [cite:vlg. stattdessen @papilloud18:_einfü_analy_texten_topic_model]. Den methodologischen Zusammenhang wollen wir dennoch kurz präzisieren, in dem wir die folgende Topic-Modellierung verwendet haben, weil dieser Zusammenhang in unserem Fach unterbelichtet bleibt. Wir haben diese Modellierung im Rahmen von Workflows hergestellt, die dem Ansatz vom /literate programming/ folgen, nach dem Anwendungen so geschrieben werden, damit sie jeder Mensch versteht. In der Praxis bedeutet dies, dass ein Text (ein Bericht, ein Aufsatz, ein Buchkapitel usw.) geschrieben wird, der die methodologischen bzw. technischen Elementen enthält (wie etwa den Code für die Ausführung von unterschiedlichen Anwendung zur Herstellung von Statistiken, Tabellen, Graphiken usw.), die im Rahmen von diesem Text verwendet werden. Ein solcher Text bzw. eine solche Textdatei kann anschließend nicht nur als Bericht/Aufsatz, sondern auch als Anwendung benutzt werden. Sie kann ausgeführt werden, und dabei führt sie weitere Anwendungen aus, die für die Ausführung des Codes in der Datei notwendig sind. Damit wird eine vollständige Transparenz und Reproduzierbarkeit der unterschiedlichen Stufen erhalten, die zu einem Ergebnis geführt haben, das der Text veröffentlicht. Im Rahmen von solchen Workflows harmonieren qualitative sowie quantitative Daten im Dienst der wissenschaftlichen Untersuchung, dass eine Textdatei repräsentiert und aufgrund ihres Formates als Textdatei überall entweder als reine Textdatei oder als Anwendung verwendbar ist.

In dem Kontext dieses Vorhabens haben wir als Grundlage der Untersuchung zur Frage der Aufmerksamkeit von Politik in Deutschland auf die Fragen, die Rechenzentren stellen, die Parlamentsdokumente der Bundesrepublik Deutschland und die Protokollen des Bundestages herangezogen. Diese Dokumente können an der folgenden Adresse als XML-Dateien, mit einer API oder nach handwerklicher Suche heruntergeladen werden: https://www.bundestag.de/services/opendata. Ein Github-Konto bietet diesen Datensatz als R-Objekt an: https://github.com/benjaminguinaudeau/tidybundestag. Inhaltlich liefern diese Dokumente unterschiedliche Informationen zu den politischen Entscheidungen, die von 1949 bis heute auf Bundesebene getroffen werden. Bezogen auf das Thema des Vorhabens in dieser Arbeit erlauben sie eine Rekonstruktion der thematischen Zusammenhänge, in denen die Rechenzentren in Deutschland von der Politik wahrgenommen wurden. Wir haben uns auf die XML-Dateien bezogen, die wir in Textdateien umgewandelt haben. Wir haben diese Dateien mit den letzten Parlamentsdokumenten und Protokollen (bis Juni 2025) vervollständigt. Anschließend haben wir die Dokumente nach Wahlperioden klassifiziert, selektiert und nach Datum, Sprecher, politischer Partei vom Sprecher und politischem Status des Sprechers (etwa Ministerpräsidenten) zerlegt. Bei der Selektion sind wir zuerst mit einem breiten Ansatz vorgegangen, nach dem wenn das Wort "Rechenzentr" zumindest einmal in einem ganzen Dokument erscheint dieses Dokument in die Analyse einfließt. Weil wir mit diesem Ansatz sehr viele Dokumente generiert haben, die mit dem Thema Rechenzentrum lose verbunden waren, haben wir den Ansatz enger gefasst und nur die Teile von Dokumenten bzw. die einzelnen Wortmeldungen von Sprechern ausgewählt, die sich unmittelbar mit dem Thema der Rechenzentren beschäftigt haben. Mit diesem letzten Ansatz konnten wir eine verdichtete Abbildung der Entwicklung der Themen zum Oberthema "Rechenzentrum" im Bundestag in der Zeit erhalten [[fig:Abb. 1]].

#+CAPTION: Entwicklung der Themen zum Oberthema "Rechenzentrum" im Bundestag
#+NAME: fig:Abb. 1
[[./Trends7T.pdf]]

Die Abbildung 1 zeigt einerseits, dass die Debatten im Bundestag das Thema Rechenzentren aus verschiedenen Perspektiven behandeln, die in der Zeit unterschiedlich bedeutsam sind. In Bezug auf die Aussage von Aebischer kann gesagt werden, dass die Politik in Deutschland auf Rechenzentren schon aufmerksam war. Aber wenn wir das Thema Energie fokussieren, sieht man dann, dass es nur im Themenkomplex Energie, Wirtschaft, Optimierung, Nachhaltigkeit behandelt wird und nur zwischen 1995 und 2000 ein Leitthema der Debatten im Bundestag gewesen ist. Davor steckt dieser Themenkomplex hinter dem Thema des Datenschutzes, und danach verliert es an Bedeutung im Vergleich zu den Themen der Forschung, der Wissenschaft und der Bildung sowie der Apotheken und der Medizin (in diesem Zusammenhang insbesondere in Bezug auf die Verbesserung der automatischen Verwaltung von Rechnungen und der entsprechenden Zahlung dieser Rechnungen). Am Ende der Zeitspanne herrschen insbesondere die Themen der nationalen Verteidigung und der Forschung und Bildung in Bezug auf die Veränderung der internationalen Lage (Konflikt zwischen Russland und der Ukraine, Covid-19 und Entwicklung der Fernarbeit, Einsatz des Blockchains zur Verschlüsselung von Transaktionen und Kommunikationen, Durchbruch der künstlichen Intelligenz).

Wenn wir zurück auf den Themenkomplex Energie, Wirtschaft, Optimierung, Nachhaltigkeit in den Jahren 1995-2000 schauen, dann sehen wir nur eine Erwähnung der Energie im Rahmen von Rechenzentrum in Bezug auf den FCKW-Halon-Verbot, die von Frau Monika Ganseforth (SPD) am 16.03.1995 aufgegriffen wird. Frau Ganseforth bedauert, dass dieser Verbot u.a. für Rechenzentren und EDV-Anlagen noch nicht gelte. Aebischer hat dann Recht, wenn er sagt, dass die Strom- und im Allgemeinen die Energiefrage bei Rechenzentren in den 90er Jahren von der deutschen Politik kaum berücksichtigt wurde. In den Debatten des Bundestags bleibt es so fast bis heute, wie unsere Abbildung nahelegt, die jedoch eine leichte Wiederbelebung vom Themenkomplex Energie, Wirtschaft, Optimierung, Nachhaltigkeit ab 2022-2023 zeigt. Wenn wir diese letzte Zeitspanne fokussieren, merken wir, dass die Debatten über die Rechenzentren sich um die Frage drehen, ob Rechenzentren als Hebel einer grünen Ökonomie fungieren könnten, die von wissenschaftlichen Innovationen unterstützt wären. Diese Frage lässt jedoch die deutsche Politik uneins und damit für die Betreiber von Rechenzentren in Deutschland und ihre Partnern in der Wirtschaft und in der Wissenschaft schwer mit einzubeziehen bzw. zu beeinflussen. Dieser Aspekt der jüngsten Debatten um die Rechenzentren im deutschen Bundestag wollen wir im folgenden vertiefen, weil er ein bedeutsames Problem betrifft, das die Rechenzentren begleitet hat und immer noch begleitet: Das Dilemma Energieverbrauch vs. Energieeffizienz.

* Energieverbrauch und Energieeffizienz

Dass Rechenzentren für die Politik an Bedeutung im Vergleich zu den 70er Jahre gewonnen haben, ist unumstritten. Im Jahre 1978 konnte man im Protokoll des Bundestages am 28.09. die Auseinandersetzung zwischen Paul Laufs (CDU) und Erwin Stahl (SPD) zu den Rechenzentrum lesen, die wir im Folgenden in Kurzform wiedergeben: "(Laufs) Aus welchen Gründen schlägt die Bundesregierung vor, die Mittelzuweisungen aus dem Haushalt für Forschung und Technologie für die Finanzierung von regionalen Rechenzentren gegenüber dem Plan des 3. DV-Programms 1976-79 drastisch zu kürzen?"; "(Stahl) Herr Kollege Dr. Laufs, es trifft nicht zu, daß die Mittel für die Finanzierung von regionalen Rechenzentren gegenüber dem Plan des 3. DV-Programms drastisch gekürzt worden sind. Im Zeitraum von 1976 bis 1979 werden 142 Millionen DM bereitgestellt werden; das sind 15 % weniger als ursprünglich geplant. Die Mittel haben bisher ausgereicht und werden auch im Jahre 1979 ausreichen"; "(Laufs) Herr Staatssekretär, sind Ihnen die Befürchtungen bei den regionalen Rechenzentren bekannt, daß die deutsche rechenintensive Forschung gegenüber der Industrie und dem Ausland ins Hintertreffen geraten wird, weil die regionalen Großrechenanlagen hinsichtlich ihres PreisLeistungs-Verhältnisses und auch ihrer Rechenleistung durch diese Politik der Bundesregierung nicht auf dem Stand der Zeit gehalten werden können?"; "(Stahl) Derartige Äußerungen, Herr Kollege Dr. Laufs, sind mir nicht bekannt. Ich darf nochmals darauf aufmerksam machen, daß für DV-Beschaffungen im Regionalprogramm bis zum 31. Dezember 1977 einschließlich des DV-Sonderprogramms insgesamt 290,6 Millionen DM ausgegeben worden sind. Weitere Klagen sind nicht bekannt". 45 Jahr später ist für die deutsche Politik unumstritten, dass es angesichts der wachsenden Bedeutung von Rechenzentren in der ganzen Gesellschaft in Rechenzentren investiert werden muss. Aber vollkommen ausgeräumt sind die Kontroversen nicht, sondern sie betreffen eine andere Frage, die an der umstrittenen Schnittstelle ökonomisches Wachstums und ökologischer Bilanz gestellt ist. Das Protokoll des Bundestages zur Sitzung vom 17.05.2023 zeigt exemplarisch, wie die Politik uneins in Bezug auf Rechenzentren reagiert. In seinem Redebeitrag im Bundestag stellt Uwe Kamann (LKR) die folgende Frage, die eine Debatte zu den Rechenzentren in dieser Sitzung hervorrufen wird: "Sie fordern von der Bundesregierung, den Stromverbrauch für den Ausbau des 5G-Netzes und die Auswirkung dessen auf Rechenzentren zu validieren. Wenn diese dann höher ausfallen, als es ideologisch akzeptierbar ist, was ist dann? Wollen Sie dann wie beim Schadstoffausstoß des Diesels willkürliche Grenzwerte festlegen, oder wollen Sie gar ganze Rechenzentren stilllegen?". Dazu äußern sich Timon Gremmels und Falko Mohrs (SPD) sowie Dieter Janecek (Bündnis 90/Die Grünen), um zu sagen, dass Rechenzentren auch im Rahmen der Energiewende berücksichtigt werden muss, weil sie im Rahmen der Debatten zur Digitalisierung eine wichtige Rolle spielen. Hansjörg Durz (CSU) antwortet darauf, dass diese Maßnahmen schon seit 2022 vorgesehen waren, und bald Teil von einem "Digitalagenda" sein werden: "Viele Ihrer Forderungen sind darin bereits enthalten. Auch die Forderung nach energieeffizienteren digitalen Produkten und der Nutzung der Abwärme von Rechenzentren ist eine Forderung, die in Papieren der Union aus den vergangenen Jahren zu finden ist". Nichtsdestotrotz werfen die Linke durch Frau Anke Domscheit der Regierung vor, dass eine solche Agenda nicht transparent genug ist.

In den Debatten im Bundestag kommt deutlich zum Vorschein, dass es daran gezweifelt wird, wie Energieeffizienz gelingen kann, ohne dass eine solche Energieeffizienz zu weniger Energieleistung führen würde, was sich Deutschland nicht erlauben kann. Falko Mohrs erinnert in der Sitzung des Bundestages am 17.05.2023, dass dieser Punkt schon 2008 im Rahmen "der Beschaffungsstrategie des Bundes verankert [wurde]". Dies lässt sich auf der Grundlage der Protokolle des Bundestages nicht wieder finden, zumal es keine eine Beschaffungsstrategie des Bundes gibt, sondern mehrere (etwa für Raumfahrttechnologien in der Partnerschaft mit der EU, für Rohstoffe und Energiequellen oder für Autos), die seit 1983 im Bundestag kontroverse diskutiert werden aber das Thema Rechenzentren nicht unmittelbar betreffen. Die Erwähnung vom Jahr 2008 ist dennoch interessant, weil anschließend an der IT-Messe CeBIT entwickelt sich in der Sitzung des Bundestages am 05.03.2008 eine Debatte einerseits zur Unterstützung der Forschung in den IT-Bereichen und andererseits zur ökologischen Maßnahmen, die in Bezug auf IKT ergriffen werden sollten. Zum ersten Punkt äußert sich Lothar Bisky (Die Linke), der besonders bedauert, dass die Bundesregierung die Forschung in den IT-Bereichen grundsätzlich als nur von den IT-Industrien getrieben sieht, und er plädiert stattdessen für mehr Unterstützung der wissenschaftlichen Forschung: "IT-Forschung ist nach dem Verständnis der Bundesregierung daher vorrangig ein Programm zur Subventionierung von Informations- und Kommunikationstechnologie in ausgewählten Anwendungsbereichen, vor allem Automobilbau, Telekommunikation, Logistik und Medizintechnik. (...) Darauf allein, Herr Tauss, lässt sich die Basis für eine Informations- und Wissensgesellschaft nicht gründen. (...) Den Hochschulen kommt auf diesem Feld eine wichtige, wenn nicht entscheidende Aufgabe für die Zukunft zu". Wissenschaftliche statt industrielle Forschung ist deshalb wichtig, weil dort befindet sich das Potential zur Herstellung von grüneren Produkte: "Nicht umsonst gibt es 'Green IT', ein Schwerpunktthema der CeBIT". Heinz Schmitt (SPD) verlängert die Überlegung von Bisky: "Laut BUND ist der Stromverbrauch von IT-Geräten für 43 Prozent der CO2-Emissionen in Deutschland verantwortlich. Handy, Computer, Fernseher, moderne Informationstechnik benötigt immer mehr Energie. (...) Um den Energieverbrauch vom Wirtschaftswachstum zu entkoppeln, müssen wir quer durch alle Wirtschaftsbereiche energieeffiziente Produkte konstruieren, produzieren, nutzen und - ein ganz wichtiger Punkt - recyclen. Und dies ist nur mit innovativer Hightech machbar. Moderne Informationstechnik und Umweltschutz müssen also miteinander
kombiniert werden. Davon profitiert die Umwelt; mit Green IT lässt sich aber auch viel Geld verdienen bzw. viel Geld einsparen". In diesem Zusammenhang erwähnt Schmitt die Expertise vom Fachverband BITKOM (heute einem Think-Thank) im Bereich IT, der zusammen mit der unabhängigen und gemeinnützigen Forschungseinrichtung Borderstep (Berlin) vom Bundesministerium für Umwelt, Naturschutz, Bau und Reaktorensicherheit (BMU) bestellt wird, um die IT-Landschaft in Deutschland und im internationalen Vergleich darzustellen und sie insbesondere in ökologischer Hinsicht zu betrachten. An diesem Punkt kommen wir zum Theorieansatz zurück, den wir in Bezug auf Sackmanns Machtbalance eingeführt haben.

Im Rahmen der Debatten zu Rechenzentren und am Beispiel von Deutschland sieht man die Entwicklung von einer Machtbalance in der Einflussstrategie, die Betreiber von Rechenzentren zusammen mit IT-Unternehmen und mit der Unterstützung der Forschung im Bereich der Herstellung von Rechnern und den damit verbundenen innovativen digitalen Technologien mit dem Ziel entwickeln, die Politik mit einzubeziehen und somit sie in der Unterstützung der Verbreitung von den medialen Verhältnissen zu bewegen, die solche Rechenzentren in der Gesellschaft verbreiten. Die Politik spielt mit, um das Bild des Fußballs Elias' zu übernehmen, versucht aber diesen Einfluss zu umkehren bzw. bestellt eine eigene Expertise aus der Wissenschaft zusammen mit spezialisierten Verbänden aus der Wirtschaft, um genau zu wissen, was Rechenzentren sind und ob sie ohne weiteres in ihrer Verbreitung unterstützt werden sollten. Die Machtbalance versteht sich dann auf beiden Seiten nicht als Blockade, sondern als die Erprobung der Zuverlässigkeit von Allianzen, die den (Kollektiv)Akteuren bevorstehen. In diesem Zusammenhang spricht die Theorie der Relation von Satellisierungsstrategien, weil sie soziale Asymmetrien nicht unmittelbar in Macht übersetzt, sondern sie als Problem der Zusammenstellung von Verhältnissen in der Durchsetzung von einer Ordnung von Verhältnissen versteht. Wenn eine Ordnung von Verhältnissen andere Ordnungen von Verhältnissen in der Gesellschaft (und die damit betroffenen Akteure und Instanzen) satellisiert bzw. in der Unterstützung der eigenen Verbreitung zu bringen versucht, dann trifft diese Ordnung von Verhältnissen unvermeidlich auf Schwierigkeiten. Diese Schwierigkeiten betreffen in erster Linie die Einbettungs- bzw. Einschreibungskontexten, deren Entwicklung erschwert wird. Nehmen wir nochmal das Beispiel des Fußballs. Ein Fußballspiel mit zwei Mannschaften erlaubt beide Mannschaften, Strategien und Taktiken gut zu planen, um das Spiel zu optimieren. Die Einbettung von Verhältnissen in Bezug auf die eigenen Spieler in der Mannschaft und auf die Gegner der anderen Mannhschaft bleibt überschaubar. In der Welt des Fußballs gibt es aber keine Meisterschaft mit nur zwei Mannschaften, sondern es gibt mehrere Mannschaften, was die Einbettung von Verhältnissen nicht nur in Bezug auf die Spieler erschwert, sondern auch in Bezug auf die Instanzen "Klubs" und "Wettkämpfe". Klubs sind unterschiedlich aufgestellt, haben deshalb unterschiedliche Prioritäten in Bezug auf Ergebnisse, auf die Finanzierung der Rekrutierung von Spielern und vom Coach, auf die Aufstellung von Infrastrukturen für das Training. Dies schlägt sich in den Trainingsablauf und schließlich in die Strategien und Taktiken nieder, die die Entwicklung des eigenen Spiels und die Anpassung am Spiel der anderen Mannschaften beeinflusst. Dies variiert ebenfalls nach den Wettbewerben, die wahrgenommen werden, und die von der Optimierung der eigenen Position in der eigenen Liga gehen, bis zur Teilnahme an internationalen Wettbewerben.

Bei Rechenzentren ist diese Komplexität um so präsenter, als ihre Verbreitung in der Informationsgesellschaft wie Selbstverständlich bzw. unwiderstehlich erscheint -- zumindest in den Augen der Akteure, die eine solche Satellisierung der Gesellschaft durch Rechenzentren durchbringen wollen, die sich im Schatten großer Medienkonzerne wie Apple, Microsoft, Alphabet, Meta oder Amazon profilieren. Für die deutsche Politik ist dennoch wichtig, so unwiderstehlich Rechenzentren sein mögen bzw. so attraktiv sie sein können, dass besonders bei Fragen des Energieverbrauchs und der Energieeffizienz andere gesellschaftliche Instanzen sowie Akteure in der Gesellschaft dieser Attraktivität von Rechenzentren nicht aufgeopfert werden. Energie brauchen nicht nur Rechenzentren, und Energie hat einen Preis im Sinne des Verbrauches von ökologischen Ressourcen sowie im Sinne der Weltverschmutzung und des Klimawandels. Dabei geht es für die deutsche Politik auf die Repräsentation bzw. die gerechte Verteilung von Energie mit Blick auf Verbrauch, Preise und Natur. Oder anders gesagt: es geht ihr darum, Rechenzentren nicht abzubauen, sondern ihre Attraktivität im Dienst dieser Repräsentation von Energie in der Gesellschaft zu bringen und damit die eigene Repräsentation in der Gesellschaft zu stärken. Die deutsche Politik versteht entsprechend Rechenzentren als Umverteilungsproblem bzw. als Problem der Umverteilung von Energie und der Gewichtung von medialen Verhältnissen in der Gesellschaft im Vergleich zu anderen Ordnung von Verhältnissen und anderen Formen des Umgangs mit Energie. Sie braucht entsprechende Instanzen, die an dieser Umverteilung arbeiten, weshalb im Jahre 2007 das BMU einen "Fachdialog (...) zum Thema 'Zukunftsmarkt ‚grüne’ Rechenzentren'" mit Partnern aus der Wissenschaft (Oldenburg Center for Sustainability Economics and Management der Universität Oldenburg, dem Fraunhofer Institut für System- und Innovationsforschung in Karlsruhe sowie dem Institut für Zukunftsstudien und Technologiebewertung) eröffnet [cite:@fichter07:_zukun_energ_rechen, I]. In diesem Zusammenhang beauftragt es Borderstep mit einem Bericht zur Frage der Rechenzentren und deren Energieeffizienz, der im selben Jahr veröffentlicht wird.

* Energieeffiziente Rechenzentren

Ein Merkmal der politischen Wahrnehmung von Technologien nicht nur in Deutschland, sondern in Europa und zum Teil auch in den Vereinigten Staaten -- kommen diese Technologien aus dem Bereich der Wissenschaft oder aus dem Bereich der Medien -- besteht darin, dass sie mit der ökonomischen Semantik codiert werden. Im Rahmen des Berichts von Borderstep zu den Rechenzentren kommt es unmittelbar vor: "Besonderes im Blickpunkt stehen dabei die Entwicklung der Wettbewerbsfähigkeit deutscher und europäischer Unternehmen im internationalen Vergleich, ihr Umfeld sowie Ansatzpunkte für eine Stärkung des deutschen und europäischen Innovationssystems" (ebd.). Dieses Merkmal ist zum einen typisch von der politischen Wahrnehmung der Technologien in Bezug auf den politischen Diskurs, der seit den 70er Jahren in den industriellen Gesellschaften die Partnerschaft zwischen Wissenschaft und Wirtschaft zur Entstehung einer Wissensökonomie fördern will. Zum zweiten spiegelt es die Grundlage der Einflussstrategie der Politik in der Gesellschaft wider, die sich die Expertise in der Wissenschaft holt, um gesellschaftliche Probleme zu dimensionieren, während sie Lösungsvorschläge aus der Wirtschaft erwartet, um ihre eigene Handlung nicht nur formal in rechtlichen Regeln und Normen, sondern auch konkret in Dienstleistungen und Produkten abbilden zu lassen. Bei dieser Strategie der Satellisierung von Wissenschaft und Wirtschaft, die wir bereits im Rahmen von Hochtechnologie beschrieben haben [cite:@monoPapilloud2012;@monoPapilloudSchultze2023], spielen Instanzen der Umverteilung in der Politik eine wichtige Rolle. Wie Instanzen der Umverteilung in anderen Bereichen der Gesellschaft tragen sie zur Erkennung von Problemen zwecks der Formulierung von Vorschlägen für die Orientierung von Akteuren und gesellschaftlichen Instanzen in Einbettungskontexten bzw. zu ihrer neuen Orientierung und zur neuen Strukturierung von solchen Einbettungskontexten. Im speziellen Bereich der Politik arbeiten solche Instanzen an der Unterstützung der politischen Repräsentation in der Gesellschaft bzw. an der Attraktivität von Politik für die ganze Gesellschaft. Sie messen, wiegen, vergleichen, beziffern, berechnen die Realität bzw. sie übertragen sie in Zahlen, Formeln, Indikatoren und geben somit dem Unbekannten Formen. Rechenzentren werden deshalb nicht fälschlicherweise als ökonomischen Themenkomplex wahrgenommen, sondern aufgrund der Satellisierungsstrategie, damit sich die Politik mit Unterstützung von ihrer Instanzen und darunter insbesondere ihrer Instanzen der Umverteilung in der Gesellschaft verbreitet. Dies soll dazu beitragen, dass "das Potential von einem solchen Markt besser benutzt wird" (ebd., 1).

Die Wahrnehmung von Rechenzentren als Markt soll in erster Linie dabei helfen, das Problem der Definition von Rechenzentren zu lösen: "Bislang gibt es keine allgemein gültige Definition des Begriffs 'Rechenzentrum' bzw. 'data centre'" (ebd. 5). Mit dem Wort 'Rechenzentrum' wird zwar eine Verbindung zwischen Rechnern und Räumlichkeiten bezeichnet, aber Rechenzentrum ist nicht gleich Rechenzentrum. Deshalb orientiert sich zuerst die Definition von Boderstep im Einklang mit der jüngsten Expertenliteratur in den Vereinigten Staaten an der Schnittstelle zwischen Rechentechnik und Räumlichkeit, um sie dann zu beziffern und somit den Begriff 'Rechenzentrum' zu präzisieren. Demnach gelten als Rechenzentren diejenige Zusammenstellungen von Duzenden bis Tausenden Servern in Räumlichkeiten von mindestens 50 bis mehr als 500 Quadratmeter (ebd., 5-6). Im Sinne von einer technischen Definition bedeutet diese räumlich orientierte Definition von Rechenzentren, dass Rechenzentren gelten als solche, wenn sie Server, Speicher, die Netzwerkausrüstung, die unterbrechungsfreie Stromversorgung, die Stromverteilung sowie die Klimatisierung und die Kühlung voraussetzen (ebd., 8). Rechenzentren sind entsprechend kein Markt, sondern eher ein Markt der Märkte, die nicht auf IT-Produkte verweisen, sondern auf Dienstleistungen im Bereich von Telekommunikationen, auf die Halbleiterindustrie, auf die Stromindustrie, auf das Bauwesen bis hin zu Startups und Spin-offs, bis zu Bereichen außerhalb der Wirtschaft wie der wissenschaftlichen Forschung in sehr unterschiedlichen Disziplinen (Chemie, Physik, Optik, Ingenieurwesen, Informatik usw.). Eine solche Definition von Rechenzentren grenzt also einerseits ab, was als 'Rechenzentren' gilt, damit nicht alles was mit Rechnern verbunden ist in diese Definition einfließt. Dies funktioniert jedoch eher rhetorisch als analytisch.

Kleine 'Rechenzentren' (wenige Rechner in Räumlichkeiten von weniger als 50 Quadratmeter), die von der Definition der Rechenzentren ausgeschlossen werden, tragen auch zum Problem der Energieeffizienz von größeren Rechenzentren bei, die diese Definition fokussiert. Denken wir etwa nur an den Zugang zum Internet, der den Anschluss von kleinen Servern mit größeren Servern voraussetzt. Aber wenn man Aebischer folgt, dann versteht man, dass das Problem auch grundsätzlicher ist: "Ideally, energy efficiency of a data centre should be measured in terms of energy consumption per unit of service delivered to the customer. However, there exists no commonly agreed method to measure the service provided by a data centre" [cite:@aebischer03:_energ, 437]. Dazu erschwert die technologische Entwicklung die Etablierung von einem Referenzwert der Energieeffizienz von Rechenzentren, der erlauben würde, genaue Messungen vom Energieverbrauch und Energieverluste in Rechenzentren zu erhalten. Aebischer schlägt deshalb einen "Coefficient of Energy Efficiency (CEE)" auf der Grundlage von zwei Richtwerten C1 und C2 vor, wo CEE = C1*C2 (ebd.). C1 misst die Effizienz der Infrastruktur von einem Rechenzentrum und C2 die Effizienz der Rechentechnik in dieser Infrastruktur (ebd., 438). Nach Aebischer kann der Wert von C1 unmittelbar erhalten werden. Dagegen bleibt der Wert von C2 schwer und nur mittelbar erhältlich (Unterschied zwischen dem gesamten Strom, der in das Rechenzentrum kommt, und dem Strom, der dabei verloren geht; ebd.). Deshalb bleibt der CEE als Indikator der Messung von der Energieeffizienz der Rechenzentren unbefriedigend, und er kann nicht unmittelbar zu Vorschlägen für die politischen Akteuren und Instanzen führen. In 2014 verallgemeinert Aebischer diese Schlussfolgerung für alle Indikatoren, die im Bereich der Messung von der Energieeffizienz der Rechenzentren eingesetzt werden, was etwa auch für den Indikator vom Green Grid /Power Usage Effectiveness/ (PUE) gilt [cite:@haas09:_usage_public_repor_guidel_green], der seit 2009 weltweit eingesetzt wird, um die Energieeffizienz von Rechenzentren zu messen [cite:@aebischer14:_energ_deman_ict, 11]. Die positive Seite von dieser Messungsschwierigkeit ist, dass das Interesse für den Energieverbrauch gestiegen ist. Wie Aebischer zeigt, sind es seitdem mehrere Initiativen weltweit (die Akzentsetzung auf die Virtualisierung von Dienstleistungen bis hin zu /cloud computing/, die Initiative /Energy Star/ in den Vereinigten Staaten, den europäischen /Code of Conduct for Data Centers/, die Initiative /CRC Energy Efficiency Scheme/ in Großbritanien; ebd., 12) die Ergriffen worden sind, um mehr Erkenntnis zum Stromverbrauch von Rechenzentren in der Hoffnung zu sammeln, dass in der nahen Zukunft energieeffizientere Rechenzentren gebaut werden können.

Borderstep folgt diesem Ansatz auch und versucht unmittelbar im Anschluß an seinem ersten Bericht in einem zweiten Bericht in Begleitung von BITKOM die Frage des Energieverbrauchs bzw. der Energieeffizienz von Rechenzentren auf den Grund zu gehen. Wie Borderstep in seinem ersten Bericht sagt: "Für die Ermittlung und das Monitoring des Energieverbrauchs von Rechenzentren ist die Frage zentral, wie viele Rechenzentren welchen Typs und welcher Größe es gibt. Hierzu existieren bis dato weder in der Wissenschaft noch bei den Branchenverbänden oder den einschlägigen Marktforschungseinrichtungen Statistiken" [cite:@fichter07:_zukun_energ_rechen, 8]. Deshalb wird 2010 versucht, eine materielle Bestandaufnahme von der geschätzten Anzahl an Rechenzentren durchzuführen. Dabei wird zwar die eingeschränkte Definition der Rechenzentren von 2007 wieder erwähnt [cite:@hintemann10:_mater_rechen_deuts, 13], aber sie wird für die Schätzung der Anzahl an Rechnerzentren nicht weiter verwendet, weil es in diesem Zusammenhang darum geht, alles, was Server enthält, im Blick zu nehmen. Oder anders formuliert: Borderstep orientiert sich nicht mehr nach der wirtschaftlichen Semantik zur Codierung vom Rechenzentren als wichtigem Markt, sondern er argumentiert jetzt auf der Grundlage von einem dezidierten technischen empirischen Ansatz. Deshalb werden auch kleinere 'Rechenzentren' bzw. 'Serverschränke' berücksichtigt. Mit diesem Ansatz kommt Borderstep auf eine Anzahl von 53170 Rechenzentren für Deutschland im Jahr 2010, die unterschiedlich groß sind und ein bisschen mehr als 1.280.000 Mio. Servern entsprechen (ebd., 25). Dann erfolgt die Schätzung vom Energieverbrauch dieser Rechenzentren insbesondere in Bezug auf die Einzelkomponenten der Server- und Speichertechnik mit Einbezug der Netzwerktechnik unter der Annahme, dass die Netzwerktechnik 10% der Servertechnik entspricht (ebd., 34). Diese Daten stellen die Grundlage für die Prognosen des Energieverbrauches von Rechenzentren in Deutschland bis 2015. Diese Prognosen orientieren sich nach den Szenarien aus dem Bericht der /Lawrence Berkeley National Laboratory/ (LBNL) in Zusammenarbeit mit der /U.S. Environmental Protection Agency/ (EPA) zur Veranschaulischung von möglichen Trends beim Energieverbrauch von Rechenzentren in den Vereinigten Staaten [cite:@brown08:_repor_congr_server_data_center_energ_effic, 8-10;@hintemann10:_mater_rechen_deuts, 85-90]. Übersetzt und vereinfacht -- LBNL spricht von fünf Szenarien, Borderstep bildet zwei davon -- kommt Borderstep zum Schluss, dass ohne Veränderungen im Vergleich zur vergangenen Entwicklungen der Rechenzentren ihrer Energieverbrauch auf 40% von 2008 bis 2015 steigen sollte (Szenario "Business as usual"). Wenn aber die "Green IT" im Rahmen von Rechenzentren entwickelt wird, dann könnte der Energieverbrauch von Rechenzentren in die umgekehrte Richtung gehen bzw. von 40% in der Zeit von 2008 bis 2015 sinken (ebd., 90). Diese Schätzung von +- 40% ist im Vergleich zur Berechnung von LBNL optimistischer in Bezug auf den Szenario "Business as usual" (LBNL sieht eine Verdoppelung des Energieverbrauches von 2006 bis 2011). Sie ist fast identisch für den Szenario "Green IT". Ist eine solche relative Übereinstimmung der Prognosen von LBNL und Borderstep ein Beleg dafür, dass die Schätzung vom Energieverbrauches bzw. der Energieeffizienz von Rechenzentren an Genauigkeit gewonnen hat? Wenn man Aebischer folgt, ist dies mehr ein Beleg dafür, dass es "less a sign of accuracy than the consequence of using the same or similar data and making similar assumptions" [cite:@aebischer14:_energ_deman_ict, 21]. Entsprechend bleiben die meisten in ihren Schlussfolgerungen vorsichtig: Energieeffizienz kann zwar erreicht werden, aber wie und wann sie erreicht werden kann, hängt von sehr vielen Faktoren ab und deshalb bleibt offen. Für die Machtbalance im Sinne der gegenseitigen Satellisierung von Politik durch Rechenzentren und von Rechenzentren durch Politik hat es Konsequenzen.

* Folgen für die Machtbalance

Aus den unterschiedlichen Berichten zur Energieeffizienz von Rechenzentren ergeben sich zwei wichtigen Erkenntnissen. Zum einen ist und bleibt die Messung vom Energieverbrauch bzw. von der Energieeffizienz von Rechenzentren nicht nur nicht einheitlich, sondern auch unvollständig. Aebischer merkte im Jahr 2009 an: "Bis heute hat sich aber noch keine Organisation auf ein Messkonzept einigen können, das erlauben würde, die Energieeffizienz von verschiedenen Rechenzentren verlässlich miteinander zu vergleichen. Es ist auch noch nicht gelungen, ein umfassendes Mass für die Energieeffizienz der Data Centres zu definieren" [cite:@aebischer09:_energ_rechen, 18]. Im Jahr 2014 behandelt er im Detail die unterschiedlichen Messwerte und Indikatoren, die weltweit zur Messung vom Energieverbrauch der Rechenzentren entwickelt sind, seitdem es versucht wird, diesen Energieverbrauch zu messen. Davon gibt keiner eine zufriedenstellende Antwort auf die Frage "Wie viele Energie verbrauchen Rechenzentren?" [cite:@aebischer14:_energ_deman_ict]. Zur konvergierenden Feststellung kommt in Deutschland der Bericht von Murzakulova im Auftrag für das Bundesministeriums für Wirtschaft und Klimaschutz: Der PUE-Wert, der sich international durchgesetzt hat und vom /Gesetz zur Steigerung der Energieeffizienz in Deutschland 1/ (EnEfG, 2023, Absch. 4, §11) mit dem Ziel aufgegriffen wird, klimaneutrale Rechenzentren zu etablieren, wird dadurch verzerrt, dass er den Energieverbrauch von Rechenzentren nur /in vitro/ gut wiedergibt. In der Praxis kommen noch zum Betrieb der Rechenzentren "Anforderungen an Verfügbarkeiten und Redundanz von Ressourcen (...) sowie gängige Geschäftsmodelle", die der PUE-Wert nicht berücksichtigt, was ihn für die Betreiber von Rechenzentren automatisch höher mit der Folge setzt, dass diese Betreiber die angeforderten PUE-Werte vom EnEfG nicht erreichen können [cite:@murzakulova25:_stand_entwic_rechen_deuts, 100]. Deshalb entstehen Initiativen wie den /Climate Neutral Data Centre Pact/ (ein Netzwerk von Rechenzentrenbetreibern und Vereinen, das seit 2021 existiert, von der EU-Kommission unterstützt wird und im Jahr 2025 Mitglieder in der Höhe von 85% des Marktanteils von Rechenzentren europaweit einbezieht; vgl. https://www.climateneutraldatacentre.net/) mit dem Ziel, an solchen Messungen zu arbeiten, den PUE-Wert zu reformulieren und weitere Instrumente und Richtwerte wie etwa für den Verbrauch vom Wasser zur Kühlung der Rechenzentren zu entwickeln (sog. WUE-Wert für /Water Usage Effectiveness/).

Zum zweiten zeigt die Durchsetzung von einem empirischen technischen Ansatz in den Berichten zur Energieeffizienz von Rechenzentren im Auftrag der Politik in Deutschland bzw. die entsprechende Auseinandersetzung mit der technischen Komplexität von Rechenzentren die Grenzen, die die Satellisierungsstrategie der Politik erreicht. Das Problem wird immer komplexer, weil wie schon Aebischer anmerkte: "ICT products are changing their nature from owned goods to services" [cite:@aebischer14:_energ_deman_ict, 20]. Weil IKT immer mehr zu Dienstleistungen werden -- ein Trend, der vom /cloud computing/ (2014) und von der KI (2017) beschleunigt wird --, ist es in der Folge immer schwieriger, diese Dienstleistung zu einem genauen /device/ zuzuschreiben und daher mit einer bestimmten Rechenzentrenleistung zu verbinden. Um ein Beispiel zu erwähnen: Prozessoren und Mikrokontroller befinden sich längst nicht mehr nur in Rechnern, sondern in vielen alltäglichen Gegenständen (Autos, Kühlschränken, Uhren, Smartphones, Fernsehern, Rundfunkgeräten usw.), die sich nicht nur mit dem Internet verbinden lassen, sondern die auch tendenziell immer eingeschaltet sind. Dies kostet nicht nur Strom an der Seite der Endbenutzer, sondern verbraucht auch viel Strom an der Seite der Rechenzentren, ohne dass man feststellen kann, wie viel Strom es ist. Immer noch in Verbindung mit diesem technischen empirischen Ansatz: Die Miniaturisierung von technischen Komponenten der Rechner und Server ist nicht unendlich. Es ist vorgesehen, dass die Verwendung vom Silikon für die Implementierung von Transistoren auf Mikrochips um 2040 auslaufen sollte (ebd., 2). Wenn dieser Silikon von 2D Materialien mit einer Breite von drei Atomen nicht ersetzt werden kann, dann ist es ein Problem, weil die Energie, die im Bereich der IKT seit dem Ende des zweiten Weltkrieges gespart werden konnte, war von der Miniaturisierung dieser Komponenten abhängig. Mit der Steigerung der Nachfrage an IKT, die sich als historischer Trend erweist, würde dann den Energieverbrauch automatisch ansteigen. Das sind die Schwierigkeiten, die gut bekannt sind. Weitere Schwierigkeiten bleiben weniger überschaubar -- wie etwa das Problem der /life cycle/ von IKT Geräten und Komponenten, ihr /recycling/, das Problem des Softwares, das wenn nicht angepasst ebenfalls zu Erhöhung vom Stromverbrauch bei Rechenzentren führen kann. Im Allgemeinen kann das Problem so formuliert werden: Wir leben in einer Informationsgesellschaft, die uns tagtäglich von immer mehr IKT als Hardware, Software und Dienstleistungen und daher von der Rechenleistung von Rechenzentren abhängig macht.

Borderstep hatte um 2010 die große Hoffnung, dass sich das Szenario "Green IT" zumindest in Deutschland durchsetzen würde, weil er in seinem Bericht von 2012 eine sinkende Tendenz beim Energieverbrauch in Rechenzentren ab 2008-2010 verzeichnen kann [cite:@hintemann14:_rechen_deuts, 38-39]. Sie wird jedoch dadurch relativiert, dass die Finanzkrise von 2008-2009 dazu geführt hat, dass weniger Server verkauft wurden. Würden nach dieser Zeit mehr Server wieder verkauft werden, dann würde "der Energiebedarf der Server- und Rechenzentren voraussichtlich wieder ansteigen" [cite:@hintemann12:_energ_energ_server_rechen_deuts, 3]. In den folgenden Jahren bestätigt sich den stetigen Anstieg des Energieverbrauches von Rechenzentren, der deutlich höher als die ersparte Energie ausfällt, die dank der technischen Entwicklung von Komponenten für Rechenzentren und von der entsprechenden Gebäudetechnik erreicht wird [cite:@hintemann20:_rechen]. Im Bericht von Murzakulova steigt die prognostizierte Entwicklung vom Energieverbrauch steil [cite:@murzakulova25:_stand_entwic_rechen_deuts, 9]. Er hat sich innerhalb von 14 Jahren (2010-2024) verdoppelt, und diese Entwicklung sollte sich bis 2045 auf 80 Mrd. KWh/Jahr (400% mehr als 2024; ebd.) durchsetzen. Diese Prognose berücksichtigt die Maßnahmen, die bis heute ergriffen worden sind, um Energie zu sparen -- wie etwa das EnEfG-Gesetz. In Bezug auf die Karbonspur von Rechenzentren ist der Bericht optimistischer. Es gäbe eine mögliche sinkende Tendenz bei den CO2-Emissionen. Aber diese Tendenz setzt voraus, dass der Energiemix erfolgreich funktioniert bzw. dass die Rechenzentren mit hauptsächlich (idealerweise: nur) erneuerbaren Energien laufen (ebd.). Im Sinne der Machtbalance zwischen Politik und Rechenzentren setzt dies an der Seite der Politik voraus, dass alle Akteure entlang ihrer Satellisierungstrategie bzw. dass Wissenschaftler wie Unternehmer besser miteinander und mit regulatorischen Instanzen der Politik arbeiten, damit das Ziel der klimaneutralen Rechenzentren erreicht wird. Dass eine solche Satellisierung erschwert wird, kann an dem folgenden Beispiel auf der Ebene der /Energy Efficiency Directive/ der Europäischen Kommission (2012 mit Revisionen in den Jahren 2018 und 2023) zu klimaneutralen Rechenzentren gezeigt werden.

Im Juli 2025 veröffentlicht der /Climate Neutral Data Centre Pact/ einen kritischen Bericht zur /Energy Efficiency Directive/ der Europäischen Kommission, die in Deutschland zum EnEfG geführt hat. Der CNDCP stellt fest, dass das Ziel, klimaneutrale Rechenzentren im Jahr 2030 zu erreichen, sich weit aus der Reichweite befindet [cite:@pact25:_minim_perfor_stand_data_centr]. Die Gründe dafür bestehen zuerst in den Regularien, die von der EU und von der Politik in den Europäischen Ländern formuliert werden, die stark von der Praxis im Bereich des Betriebs von Rechenzentren abweichen [cite:@pact25:_minim_perfor_stand_data_centr, 3-4]. Die Definition von  PUE- und WUE-Werte zur Förderung der Energieeffizienz von Rechenzentren, die mehr als 500 KW/h verbrauchen, sind häufig zu niedrig gesetzt. Deshalb und abgesehen von einigen Ausnahmen können die große Mehrheit von diesen Rechenzentren in Europa solche Werte nicht erreichen, oder dann könnten sie nur noch eingeschränkt funktionieren, was zu einer Schrumpfung der Dienstleistungen führen würde, die sie in der Gesellschaft anbieten. Die PUE- und WUE-Werte gelten für kleinere Rechenzentren (Rechenzentren, die weniger als 500 KW/h verbrauchen) nicht. Jedoch sind diese kleineren Rechenzentren diejenige, die am wenigsten energieeffizient sind. Mit der /Directive/ ist es also /de facto/ nicht möglich, die Rechenzentren zu identifizieren, die am wenigsten energieeffizient sind und entsprechend umgebaut werden sollten (ebd., 6). Im Sinne von klimaneutralen Rechenzentren ist es deshalb besonders bedauerlich, weil solche kleinere Rechenzentren die große Mehrheit der Rechenzentren in Europa darstellen. Dies führt dazu, dass die Daten zu Rechenzentren, die die EU-Kommission sammelt, um ihre /Directive/ zu verbessern, sehr lückenhaft und von schlechter Qualität ist, was die /Directive/ nicht nur schwächt, sondern auch ihre adäquate Zusammenstellung mit anderen Maßnahmen der EU-Kommission zur Durchsetzung der Energieeffizienz von Rechenzentren in Europa erschwert. Dabei lässt sich die Botschaft von CNDCP eindeutig erkennen, die wie folgt zusammengefasst werden kann: Regularien zu Green IT ja, aber sie müssen nicht im Weg der Verbreitung von Rechenzentren stehen. Es gilt die Machtbalance. Ob sich Rechenzentren weltweit daran halten, diese Machtbalence zusammen mit Politik und Wissenschaft zu konsolidieren, bleibt dagegen fraglich. Inzwischen ist die Zeit von KI gestützten Technologien gekommen, die große Tech-Entwickler in Verbindung mit ihren eigenen Rechenzentren entwickeln, dafür Green IT nicht wirklich in Frage kommt. Jüngste Prognosen in diesem Bereich bieten einen andere Perspektive zur Frage des Energieverbrauches bzw. der Energieeffizienz von Rechenzentren in der Zeit von KI-Technologien, mit denen wir diesen Beitrag abschließen.

* Schlussbetrachtung

Der Bericht /AI-2027/ der Autoren Daniel Kokotajlo, Scott Alexander, Thomas Larsen, Eli Lifland und Romeo Dean beschäftigt sich mit Prognosen zur Entwicklung von KI-Technologien in der Gesellschaft [cite:@ai-2027]. KI-Technologien sind grundsätzlich Algorithmen, die Informationen aus dem Internet (Texte, Bilder, Musik usw.) für Endbenutzer von solchen Technologien je nachdem zusammenstellen, was diese Endbenutzer brauchen. In Ihrem Bericht geht es für die Autoren darum, diese Entwicklung besser zu verstehen. Deshalb arbeiten sie an der Herstellung von einer Zeitspanne zur Einschätzung der Fortschritte, die in diesem Bereich gemacht werden können, wenn man von den historischen Trends in der Entwicklung von IKT ausgeht. Sie kommen zu dem Schluß, dass in einer nahen Zukunft um das Jahr 2027 (von 2027 bis 2033) KI-Technologien immer genauer arbeiten, weil sie sich selbst immer besser korrigieren. Dies eröffnet den Weg für autonome KI-Technologien, die ohne menschliche Steuerung sondern in Netzwerk funktionieren würden, und im Laufe der Verbesserung ihrer rechnerischen Operationen zu einer Superintelligenz gipfeln würden, die überall in Geräten und Gegenständen (etwa Robotern) implementiert werden könnte. In diesem Zusammenhang spielen die Rechenzentren eine sehr wichtige Rolle, weil sie solche KI-Technologien empfangen, die auf ihren Servern laufen müssen. Wie wir es oben angemerkt haben, haben auch Anwendungen eine wichtige Bedeutung für die Frage vom Energieverbrauch bzw. der Energieeffizienz von Rechenzentren. In ihrem Versuch, die unterschiedlichen Konsequenzen der Entwicklung von KI-Technologien zu diskutieren, haben die Autoren von /AI-2027/ auch diese Frage berücksichtigt. Daraus ergeben sich zwei wichtigen Ergebnisse.

Einerseits gibt es zwischen den Rechenzentren und Betreibern von Rechenzentren die Verdeutlichung von einer Grenze zwischen Akteuren, die in der Wirtschaft unterwegs sind, und andere Akteuren von großen Techkonzernen, die ihr Geschäft mit reinen medialen Dienstleistungen zur Förderung, Kauf und Verkauf von medialen Verhältnissen mittels Geräte und Anwendungen machen (wie etwa Meta, Google, Apple). Andererseits zeigt sich diese Grenze ganz konkret in Bezug auf Rechenzentren als Verbindung von Techniken und Räumlichkeiten und in Bezug auf ihren Energieverbrauch, was die großen Techkonzerne als Umverteilungsproblem umdefinieren: Um KI-Technologien weiter entwickeln zu können, braucht man deutlich mächtigere Rechenzentren als diejenige, die zur Zeit existieren und in der Literatur als Hyperscaler erwähnt werden. Solche Hyperscaler brauchen mehr Strom, was die bestehenden Rechenzentren im Moment kaum bis gar nicht anbieten können. Wie BITKOM in seinem Bericht von 2024 anmerkt: "Für die Betreiber von Colocation-Rechenzentren besteht hier (...) das Marktrisiko, dass Hyperscaler in Deutschland wie auch in anderen Ländern dazu übergehen, ihre Gebäude selbst zu betreiben" [cite:@bitkom24:_rechen_deuts, 60] -- oder anders gesagt: Neue Rechenzentren einer ganz anderen Größenordnung werden von Techkonzernen für ihre Bedürfnisse gebaut, die sich nicht nur von gewöhnlichen Rechenzentren, sondern auch von Regularien und darunter besonders im Bereich des Energieverbrauches bzw. der Energieeffizienz abgrenzen. Im Bericht /AI-2027/ wird prognostiziert, dass solche Hyperscaler
